{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a26a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration: Wins = 4, Average Return = 0.004\n",
      "Value Iteration: Wins = 11, Average Return = 0.011\n",
      "\n",
      "\n",
      "Value Iteration performed better in terms of number of wins.\n",
      "Value Iteration performed better in terms of average return.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.utils import play\n",
    "\n",
    "def policy_iteration(policy, env, discount_factor=1.0, theta=1e-9, max_iterations=1000):\n",
    "   \n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    value_func = np.zeros(num_states)  # Initialize value function\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Policy Evaluation\n",
    "        while True:\n",
    "            prev_value_func = value_func.copy()\n",
    "            for state in range(num_states):\n",
    "                value_func[state] = sum([policy[state][action] *\n",
    "                                         sum([prob * (reward + discount_factor * prev_value_func[next_state])\n",
    "                                              for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                                         for action in range(num_actions)])\n",
    "            max_diff = np.max(np.abs(value_func - prev_value_func))\n",
    "            if max_diff < theta:\n",
    "                break\n",
    "\n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for state in range(num_states):\n",
    "            old_action = np.argmax(policy[state])\n",
    "            best_action_value = max([sum([prob * (reward + discount_factor * value_func[next_state])\n",
    "                                          for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                                     for action in range(num_actions)])\n",
    "            best_actions = [action for action in range(num_actions)\n",
    "                             if sum([prob * (reward + discount_factor * value_func[next_state])\n",
    "                                     for prob, next_state, reward, _ in env.P[state][action]]) == best_action_value]\n",
    "            new_policy = np.zeros(num_actions)\n",
    "            new_policy[best_actions] = 1 / len(best_actions)\n",
    "            policy[state] = new_policy\n",
    "            if old_action != np.argmax(policy[state]):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, value_func\n",
    "\n",
    "def value_iteration(env, discount_factor=1.0, theta=1e-9, max_iterations=1000):\n",
    "   \n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    value_func = np.zeros(num_states)  # Initialize value function\n",
    "    policy = np.zeros((num_states, num_actions))  # Initialize policy\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        prev_value_func = value_func.copy()\n",
    "        for state in range(num_states):\n",
    "            action_values = [sum([prob * (reward + discount_factor * prev_value_func[next_state])\n",
    "                                  for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                             for action in range(num_actions)]\n",
    "            value_func[state] = max(action_values)\n",
    "\n",
    "        max_diff = np.max(np.abs(value_func - prev_value_func))\n",
    "        if max_diff < theta:\n",
    "            break\n",
    "\n",
    "    # Extract optimal policy\n",
    "    for state in range(num_states):\n",
    "        action_values = [sum([prob * (reward + discount_factor * value_func[next_state])\n",
    "                              for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                         for action in range(num_actions)]\n",
    "        best_actions = np.argwhere(action_values == np.max(action_values)).flatten()\n",
    "        policy[state][best_actions] = 1 / len(best_actions)\n",
    "\n",
    "    return policy, value_func\n",
    "\n",
    "def compare_algorithms(env):\n",
    "    \"\"\"\n",
    "    Compare the performance of Policy Iteration and Value Iteration algorithms.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): OpenAI Gym environment object\n",
    "    \"\"\"\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize random policy\n",
    "    initial_policy = np.ones((num_states, num_actions)) / num_actions\n",
    "\n",
    "    # Policy Iteration\n",
    "    policy_pi, value_func_pi = policy_iteration(initial_policy, env)\n",
    "\n",
    "    # Value Iteration\n",
    "    policy_vi, value_func_vi = value_iteration(env)\n",
    "\n",
    "    # Evaluate algorithms\n",
    "    num_episodes = 1000\n",
    "    wins_pi = 0\n",
    "    wins_vi = 0\n",
    "    returns_pi = []\n",
    "    returns_vi = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_return_pi = 0\n",
    "        total_return_vi = 0\n",
    "\n",
    "        while not done:\n",
    "            action_pi = np.random.choice(num_actions, p=policy_pi[state])\n",
    "            action_vi = np.random.choice(num_actions, p=policy_vi[state])\n",
    "            next_state_pi, reward_pi, done, _ = env.step(action_pi)\n",
    "            next_state_vi, reward_vi, done, _ = env.step(action_vi)\n",
    "            total_return_pi += reward_pi\n",
    "            total_return_vi += reward_vi\n",
    "            state = next_state_pi\n",
    "\n",
    "        if reward_pi == 1:\n",
    "            wins_pi += 1\n",
    "        if reward_vi == 1:\n",
    "            wins_vi += 1\n",
    "        returns_pi.append(total_return_pi)\n",
    "        returns_vi.append(total_return_vi)\n",
    "\n",
    "    avg_return_pi = np.mean(returns_pi)\n",
    "    avg_return_vi = np.mean(returns_vi)\n",
    "\n",
    "    print(f\"Policy Iteration: Wins = {wins_pi}, Average Return = {avg_return_pi:.3f}\")\n",
    "    print(f\"Value Iteration: Wins = {wins_vi}, Average Return = {avg_return_vi:.3f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if wins_pi > wins_vi:\n",
    "        print(\"Policy Iteration performed better in terms of number of wins.\")\n",
    "    elif wins_pi < wins_vi:\n",
    "        print(\"Value Iteration performed better in terms of number of wins.\")\n",
    "    else:\n",
    "        print(\"Both algorithms performed equally in terms of number of wins.\")\n",
    "        \n",
    "\n",
    "    if avg_return_pi > avg_return_vi:\n",
    "        print(\"Policy Iteration performed better in terms of average return.\")\n",
    "    elif avg_return_pi < avg_return_vi:\n",
    "        print(\"Value Iteration performed better in terms of average return.\")\n",
    "    else:\n",
    "        print(\"Both algorithms performed equally in terms of average return.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"FrozenLake-v1\")\n",
    "    compare_algorithms(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc1947",
   "metadata": {},
   "source": [
    "## Why Value Iteration Outperforms Policy Iteration\n",
    "\n",
    "- **Efficiency in Larger State Spaces:** Value Iteration can handle larger state spaces more efficiently than Policy Iteration, making it more suitable for environments with a large number of states.\n",
    "\n",
    "- **Handling Stochastic Environments:** The stochastic nature of the FrozenLake-v1 environment, where the agent's actions have a probability of success or failure, contributes to the better performance of Value Iteration.\n",
    "\n",
    "- **Effective Handling of Stochastic Transitions:** Value Iteration can account for stochastic transitions more effectively than Policy Iteration, which relies on iteratively evaluating and improving the policy.\n",
    "\n",
    "- **Suitability for Small State Spaces:** Although the FrozenLake-v1 environment has a relatively small state space, Value Iteration's ability to directly compute the optimal value function makes it more suitable for this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d554e03",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The Policy Iteration and Value Iteration algorithms are used to find the optimal policy for a Markov Decision Process (MDP) environment.\n",
    "\n",
    "## Policy Iteration\n",
    "Policy Iteration is an iterative algorithm used in the field of reinforcement learning to find the optimal policy for a given MDP. It involves two main steps: Policy Evaluation and Policy Improvement.\n",
    "\n",
    "**Description:** The Policy Iteration algorithm alternates between policy evaluation and policy improvement.\n",
    "\n",
    "**Process:**\n",
    "1. **Policy Evaluation:** Compute the value function for the current policy. This step involves iterating over the states of the MDP and updating their values based on the expected rewards and next-state values.\n",
    "2. **Policy Improvement:** Update the policy based on the value function. This step involves examining each state and selecting actions that maximize the expected cumulative reward, according to the updated value function.\n",
    "\n",
    "## Value Iteration\n",
    "Value Iteration is another iterative algorithm used to find the optimal policy for an MDP. Unlike Policy Iteration, Value Iteration directly computes the optimal value function without explicitly maintaining a policy.\n",
    "\n",
    "**Description:** The Value Iteration algorithm iteratively computes the optimal value function until it converges to the true optimal values.\n",
    "\n",
    "**Process:** At each iteration, the algorithm updates the value of each state based on the maximum expected cumulative reward achievable from that state, considering all possible actions and their associated rewards and next states. This process continues until the values converge.\n",
    "\n",
    "**Outcome:** Once the optimal value function is computed, the optimal policy can be derived from it by selecting the action that maximizes the expected cumulative reward from each state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c133f3",
   "metadata": {},
   "source": [
    "## <span style=\"color: #3366cc; font-family: Arial, sans-serif; font-size: 36px; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); animation: fadeInOut 3s infinite;\">With Render</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.utils import play\n",
    "\n",
    "def policy_iteration(policy, env, discount_factor=1.0, theta=1e-9, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Policy Iteration algorithm to find the optimal policy for a given environment.\n",
    "\n",
    "    Args:\n",
    "        policy (np.ndarray): 2D array of size (num_states, num_actions) representing the initial policy\n",
    "        env (gym.Env): OpenAI Gym environment object\n",
    "        discount_factor (float): MDP discount factor (default: 1.0)\n",
    "        theta (float): Threshold for value function change (default: 1e-8)\n",
    "        max_iterations (int): Maximum number of iterations (default: 1000)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Optimal policy\n",
    "        np.ndarray: Value function under the optimal policy\n",
    "    \"\"\"\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    value_func = np.zeros(num_states)  # Initialize value function\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Policy Evaluation\n",
    "        while True:\n",
    "            prev_value_func = value_func.copy()\n",
    "            for state in range(num_states):\n",
    "                value_func[state] = sum([policy[state][action] *\n",
    "                                         sum([prob * (reward + discount_factor * prev_value_func[next_state])\n",
    "                                              for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                                         for action in range(num_actions)])\n",
    "            max_diff = np.max(np.abs(value_func - prev_value_func))\n",
    "            if max_diff < theta:\n",
    "                break\n",
    "\n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for state in range(num_states):\n",
    "            old_action = np.argmax(policy[state])\n",
    "            best_action_value = max([sum([prob * (reward + discount_factor * value_func[next_state])\n",
    "                                          for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                                     for action in range(num_actions)])\n",
    "            best_actions = [action for action in range(num_actions)\n",
    "                             if sum([prob * (reward + discount_factor * value_func[next_state])\n",
    "                                     for prob, next_state, reward, _ in env.P[state][action]]) == best_action_value]\n",
    "            new_policy = np.zeros(num_actions)\n",
    "            new_policy[best_actions] = 1 / len(best_actions)\n",
    "            policy[state] = new_policy\n",
    "            if old_action != np.argmax(policy[state]):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, value_func\n",
    "\n",
    "def value_iteration(env, discount_factor=1.0, theta=1e-9, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm to find the optimal policy for a given environment.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): OpenAI Gym environment object\n",
    "        discount_factor (float): MDP discount factor (default: 1.0)\n",
    "        theta (float): Threshold for value function change (default: 1e-8)\n",
    "        max_iterations (int): Maximum number of iterations (default: 1000)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Optimal policy\n",
    "        np.ndarray: Value function under the optimal policy\n",
    "    \"\"\"\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    value_func = np.zeros(num_states)  # Initialize value function\n",
    "    policy = np.zeros((num_states, num_actions))  # Initialize policy\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        prev_value_func = value_func.copy()\n",
    "        for state in range(num_states):\n",
    "            action_values = [sum([prob * (reward + discount_factor * prev_value_func[next_state])\n",
    "                                  for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                             for action in range(num_actions)]\n",
    "            value_func[state] = max(action_values)\n",
    "\n",
    "        max_diff = np.max(np.abs(value_func - prev_value_func))\n",
    "        if max_diff < theta:\n",
    "            break\n",
    "\n",
    "    # Extract optimal policy\n",
    "    for state in range(num_states):\n",
    "        action_values = [sum([prob * (reward + discount_factor * value_func[next_state])\n",
    "                              for prob, next_state, reward, _ in env.P[state][action]])\n",
    "                         for action in range(num_actions)]\n",
    "        best_actions = np.argwhere(action_values == np.max(action_values)).flatten()\n",
    "        policy[state][best_actions] = 1 / len(best_actions)\n",
    "\n",
    "    return policy, value_func\n",
    "\n",
    "def compare_algorithms(env):\n",
    "    \"\"\"\n",
    "    Compare the performance of Policy Iteration and Value Iteration algorithms.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): OpenAI Gym environment object\n",
    "    \"\"\"\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize random policy\n",
    "    initial_policy = np.ones((num_states, num_actions)) / num_actions\n",
    "\n",
    "    # Policy Iteration\n",
    "    policy_pi, value_func_pi = policy_iteration(initial_policy, env)\n",
    "\n",
    "    # Value Iteration\n",
    "    policy_vi, value_func_vi = value_iteration(env)\n",
    "\n",
    "    # Evaluate algorithms\n",
    "    num_episodes = 1000\n",
    "    wins_pi = 0\n",
    "    wins_vi = 0\n",
    "    returns_pi = []\n",
    "    returns_vi = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_return_pi = 0\n",
    "        total_return_vi = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()  # Display the environment\n",
    "            action_pi = np.random.choice(num_actions, p=policy_pi[state])\n",
    "            action_vi = np.random.choice(num_actions, p=policy_vi[state])\n",
    "            next_state_pi, reward_pi, done, _ = env.step(action_pi)\n",
    "            next_state_vi, reward_vi, done, _ = env.step(action_vi)\n",
    "            total_return_pi += reward_pi\n",
    "            total_return_vi += reward_vi\n",
    "            state = next_state_pi\n",
    "\n",
    "        if reward_pi == 1:\n",
    "            wins_pi += 1\n",
    "        if reward_vi == 1:\n",
    "            wins_vi += 1\n",
    "        returns_pi.append(total_return_pi)\n",
    "        returns_vi.append(total_return_vi)\n",
    "\n",
    "    avg_return_pi = np.mean(returns_pi)\n",
    "    avg_return_vi = np.mean(returns_vi)\n",
    "\n",
    "    print(f\"Policy Iteration: Wins = {wins_pi}, Average Return = {avg_return_pi:.3f}\")\n",
    "    print(f\"Value Iteration: Wins = {wins_vi}, Average Return = {avg_return_vi:.3f}\")\n",
    "\n",
    "    if wins_pi > wins_vi:\n",
    "        print(\"Policy Iteration performed better in terms of number of wins because it converges to the optimal policy more efficiently.\")\n",
    "    elif wins_pi < wins_vi:\n",
    "        print(\"Value Iteration performed better in terms of number of wins because it can handle larger state spaces more efficiently.\")\n",
    "    else:\n",
    "        print(\"Both algorithms performed equally in terms of number of wins.\")\n",
    "\n",
    "    if avg_return_pi > avg_return_vi:\n",
    "        print(\"Policy Iteration performed better in terms of average return because it converges to the optimal policy more efficiently.\")\n",
    "    elif avg_return_pi < avg_return_vi:\n",
    "        print(\"Value Iteration performed better in terms of average return because it can handle larger state spaces more efficiently.\")\n",
    "    else:\n",
    "        print(\"Both algorithms performed equally in terms of average return.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"FrozenLake-v1\")\n",
    "    compare_algorithms(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
