{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d819df05",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eaf7fd; padding: 20px; border-radius: 10px; margin-bottom: 20px; box-shadow: 0px 4px 15px rgba(0, 0, 0, 0.1);\">\n",
    "\n",
    "### MAB Agent Problem Formulation\n",
    "\n",
    "In the Multi-Armed Bandit (MAB) problem, the agent is confronted with a situation where there are multiple actions or \"arms\" to choose from, and each arm is associated with a specific reward. The overarching challenge is to maximize the cumulative reward over a series of trials or time steps.\n",
    "\n",
    "#### Limited Information\n",
    "\n",
    "Initially, the agent operates with limited information about the true reward distribution for each action, introducing an element of uncertainty.\n",
    "\n",
    "#### Exploration and Exploitation Trade-off\n",
    "\n",
    "The agent faces a critical trade-off between exploration and exploitation:\n",
    "\n",
    "- **<span style=\"color: green; font-weight: bold;\">Exploration:</span>**\n",
    "  Trying different actions to learn their effectiveness and gather more information about their potential rewards.\n",
    "\n",
    "- **<span style=\"color: red; font-weight: bold;\">Exploitation:</span>**\n",
    "  Choosing the action that appears to be most rewarding based on the current knowledge.\n",
    "\n",
    "#### Balancing Strategy\n",
    "\n",
    "The ultimate goal is to develop a strategy that effectively balances exploration and exploitation. This strategic balance is crucial for achieving the highest possible cumulative reward over time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d7ac9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Total Rewards for ε-Greedy (ε=0.01): 390.2\n",
      "Average Total Rewards for ε-Greedy (ε=0.3): 346.9\n",
      "Average Total Rewards for UCB (c=1.5): 376.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to simulate ε-Greedy action\n",
    "def epsilon_greedy_action(Q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Exploration: Randomly choose an action\n",
    "        return np.random.choice(len(Q_values))\n",
    "    else:\n",
    "        # Exploitation: Choose the action with the highest Q-value\n",
    "        return np.argmax(Q_values)\n",
    "\n",
    "# Function to simulate Upper-Confidence-Bound (UCB) action\n",
    "def ucb_action(Q_values, c, counts, total_steps):\n",
    "    # Calculate the Upper Confidence Bound for each action\n",
    "    ucb_values = Q_values + c * np.sqrt(np.log(total_steps) / (counts + 1e-6))\n",
    "    # Choose the action with the highest UCB value\n",
    "    return np.argmax(ucb_values)\n",
    "\n",
    "# Function to simulate the MAB problem\n",
    "def run_multi_armed_bandit(ads_clicks, epsilon, c, total_steps):\n",
    "    num_ads = len(ads_clicks.columns)\n",
    "    \n",
    "    # Initialize action values and counts\n",
    "    Q_values = np.zeros(num_ads)\n",
    "    action_counts = np.zeros(num_ads)\n",
    "    \n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(total_steps):\n",
    "        # Choose action using ε-Greedy or UCB strategy\n",
    "        if np.random.rand() < 0.5:\n",
    "            action = epsilon_greedy_action(Q_values, epsilon)\n",
    "        else:\n",
    "            action = ucb_action(Q_values, c, action_counts, step + 1)\n",
    "        \n",
    "        # Get reward from the chosen action\n",
    "        reward = ads_clicks.iloc[step, action]\n",
    "        \n",
    "        # Update action values and counts\n",
    "        action_counts[action] += 1\n",
    "        Q_values[action] += (reward - Q_values[action]) / action_counts[action]\n",
    "        \n",
    "        # Update total rewards\n",
    "        total_rewards += reward\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "# Load the Ads_Clicks dataset from CSV\n",
    "ads_clicks = pd.read_csv(\"Ads_Clicks.csv\")\n",
    "\n",
    "# Set parameters\n",
    "epsilon_values = [0.01, 0.3]\n",
    "c_value = 1.5\n",
    "total_steps = 2000\n",
    "\n",
    "# Function to run multiple simulations and compute average rewards\n",
    "def run_multiple_simulations(ads_clicks, epsilon, c, total_steps, num_simulations):\n",
    "    total_rewards_list = []\n",
    "    for _ in range(num_simulations):\n",
    "        total_rewards = run_multi_armed_bandit(ads_clicks, epsilon, c, total_steps)\n",
    "        total_rewards_list.append(total_rewards)\n",
    "    return np.mean(total_rewards_list)\n",
    "\n",
    "# Set additional parameter\n",
    "num_simulations = 10\n",
    "\n",
    "# Run simulations for ε-Greedy with different ε values\n",
    "for epsilon in epsilon_values:\n",
    "    avg_total_rewards_epsilon = run_multiple_simulations(ads_clicks, epsilon, c_value, total_steps, num_simulations)\n",
    "    print(f\"Average Total Rewards for ε-Greedy (ε={epsilon}): {avg_total_rewards_epsilon}\")\n",
    "\n",
    "# Run simulation for UCB\n",
    "avg_total_rewards_ucb = run_multiple_simulations(ads_clicks, epsilon_values[0], c_value, total_steps, num_simulations)\n",
    "print(f\"Average Total Rewards for UCB (c={c_value}): {avg_total_rewards_ucb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10f892",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 20px; border-radius: 10px; margin-bottom: 20px; box-shadow: 0px 2px 10px rgba(0, 0, 0, 0.1);\">\n",
    "\n",
    "### Comparison of Action Value Estimates to Optimal Action\n",
    "\n",
    "- **<span style=\"color: #007BFF; font-weight: bold;\">Estimation of Action Value:</span>**\n",
    "  For both the ε-greedy and UCB approaches, the action value serves as an estimate of how good each action is based on the data collected during the experiment.\n",
    "\n",
    "- **<span style=\"color: #007BFF; font-weight: bold;\">Optimal Action:</span>**\n",
    "  The optimal action is the one with the highest true expected reward.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #eaf7fd; padding: 15px; border-radius: 10px; margin-bottom: 20px; box-shadow: 0px 2px 10px rgba(0, 0, 0, 0.1);\">\n",
    "\n",
    "### ε-Greedy\n",
    "\n",
    "- **<span style=\"color: #28a745; font-weight: bold;\">When ε=0.01:</span>**\n",
    "  - With a very low exploration rate (ε=0.01), the agent primarily exploits the action that seems to have the highest estimated reward.\n",
    "  - The action value estimate tends to be closer to the optimal action.\n",
    "  - Average Total Rewards: 390.2\n",
    "\n",
    "- **<span style=\"color: #28a745; font-weight: bold;\">When ε=0.3:</span>**\n",
    "  - With a higher exploration rate (ε=0.3), the agent explores more, occasionally choosing suboptimal actions.\n",
    "  - The action value estimate might deviate from the optimal action more frequently.\n",
    "  - Average Total Rewards: 346.9\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #eaf7fd; padding: 15px; border-radius: 10px; margin-bottom: 20px; box-shadow: 0px 2px 10px rgba(0, 0, 0, 0.1);\">\n",
    "\n",
    "### UCB (c=1.5)\n",
    "\n",
    "- **<span style=\"color: #dc3545; font-weight: bold;\">Incorporating Uncertainty:</span>**\n",
    "  - UCB incorporates uncertainty in its action selection.\n",
    "  - The action value estimate is influenced by both the estimated reward and the uncertainty term.\n",
    "\n",
    "- **<span style=\"color: #dc3545; font-weight: bold;\">With Confidence Parameter c=1.5:</span>**\n",
    "  - With a higher confidence parameter (c=1.5), the agent tends to explore cautiously, giving more weight to actions that are less explored but have the potential for higher rewards.\n",
    "  - The action value estimate could be more stable and closer to the optimal action.\n",
    "  - Average Total Rewards: 376.0\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #f5f5f5; padding: 20px; border-radius: 10px; box-shadow: 0px 2px 10px rgba(0, 0, 0, 0.1);\">\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **<span style=\"color: #007BFF; font-weight: bold;\">ε-Greedy Approach:</span>**\n",
    "  Trades off exploration and exploitation, and the action value estimate can vary based on the exploration rate.\n",
    "\n",
    "- **<span style=\"color: #007BFF; font-weight: bold;\">UCB Approach:</span>**\n",
    "  Considers uncertainty explicitly, and the action value estimate tends to be more influenced by both the estimated reward and uncertainty.\n",
    "  Provides a potentially more balanced exploration-exploitation strategy.\n",
    "\n",
    "- **<span style=\"color: #007BFF; font-weight: bold;\">Performance Dependency:</span>**\n",
    "  The actual performance depends on the specific problem and parameter values.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
